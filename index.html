<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>English-Hindi Transliteration: Neural Architecture Benchmark</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 20px;
            text-align: center;
            margin-bottom: 40px;
        }

        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }

        .badge {
            display: inline-block;
            background: rgba(255,255,255,0.2);
            padding: 5px 15px;
            border-radius: 20px;
            margin: 5px;
            font-size: 0.9em;
        }

        .highlights {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 40px 0;
        }

        .highlight-card {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            text-align: center;
        }

        .highlight-card h3 {
            color: #667eea;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        .highlight-card .value {
            font-size: 2.5em;
            font-weight: bold;
            color: #333;
            margin: 10px 0;
        }

        .highlight-card .label {
            color: #666;
            font-size: 0.9em;
        }

        section {
            background: white;
            padding: 40px;
            margin: 30px 0;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        h2 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 2em;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            color: #764ba2;
            margin: 30px 0 15px 0;
            font-size: 1.5em;
        }

        h4 {
            color: #667eea;
            margin: 20px 0 10px 0;
            font-size: 1.2em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: #f5f5f5;
        }

        .best {
            background: #d4edda;
            font-weight: bold;
        }

        .image-container {
            margin: 30px 0;
            text-align: center;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }

        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .code-block {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
        }

        .code-block code {
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        .model-card {
            background: #f9f9f9;
            padding: 20px;
            margin: 15px 0;
            border-left: 4px solid #667eea;
            border-radius: 5px;
        }

        .model-card h4 {
            color: #667eea;
            margin-bottom: 10px;
        }

        .metric {
            display: inline-block;
            margin: 5px 10px 5px 0;
            padding: 5px 10px;
            background: white;
            border-radius: 5px;
            font-size: 0.9em;
        }

        .error-example {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }

        .success-example {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }

        .prediction-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .prediction-card {
            background: #f9f9f9;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #dc3545;
        }

        .comparison-table {
            background: #f9f9f9;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        .correct {
            color: #28a745;
            font-weight: bold;
        }

        .incorrect {
            color: #dc3545;
        }

        footer {
            text-align: center;
            padding: 40px 20px;
            color: #666;
            margin-top: 60px;
        }

        .btn {
            display: inline-block;
            padding: 12px 30px;
            background: #667eea;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            margin: 10px 5px;
            transition: background 0.3s;
        }

        .btn:hover {
            background: #764ba2;
        }

        ul {
            margin: 20px 0;
            padding-left: 30px;
            line-height: 2;
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8em;
            }
            
            .highlights {
                grid-template-columns: 1fr;
            }
            
            section {
                padding: 20px;
            }

            .prediction-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>English-Hindi Transliteration Benchmark</h1>
        <p>Comprehensive Evaluation of 8 Neural Architectures</p>
        <div style="margin-top: 20px;">
            <span class="badge">Seq2Seq</span>
            <span class="badge">Attention</span>
            <span class="badge">Transformer</span>
            <span class="badge">4,502 Test Samples</span>
        </div>
    </header>

    <div class="container">
        <!-- Key Highlights -->
        <div class="highlights">
            <div class="highlight-card">
                <h3>Best Accuracy</h3>
                <div class="value">43.07%</div>
                <div class="label">Transformer</div>
            </div>
            <div class="highlight-card">
                <h3>Lowest Error</h3>
                <div class="value">0.94</div>
                <div class="label">BiLSTM + Attention</div>
            </div>
            <div class="highlight-card">
                <h3>Fastest</h3>
                <div class="value">1.26ms</div>
                <div class="label">GRU</div>
            </div>
            <div class="highlight-card">
                <h3>Models Tested</h3>
                <div class="value">8</div>
                <div class="label">Architectures</div>
            </div>
        </div>

        <!-- Results Summary -->
        <section id="results">
            <h2>Results Summary</h2>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Accuracy</th>
                        <th>Params (M)</th>
                        <th>Speed (ms)</th>
                        <th>Edit Distance</th>
                        <th>Off by 1</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="best">
                        <td><strong>Transformer</strong></td>
                        <td>43.07%</td>
                        <td>5.57</td>
                        <td>15.30</td>
                        <td>1.06</td>
                        <td>1364</td>
                    </tr>
                    <tr class="best">
                        <td><strong>BiLSTM + Attention</strong></td>
                        <td>42.11%</td>
                        <td>37.28</td>
                        <td>2.53</td>
                        <td>0.94</td>
                        <td>1508</td>
                    </tr>
                    <tr>
                        <td>LSTM + Attention</td>
                        <td>39.20%</td>
                        <td>8.99</td>
                        <td>2.18</td>
                        <td>1.05</td>
                        <td>1460</td>
                    </tr>
                    <tr>
                        <td>LSTM</td>
                        <td>39.18%</td>
                        <td>7.42</td>
                        <td>1.29</td>
                        <td>1.06</td>
                        <td>1447</td>
                    </tr>
                    <tr>
                        <td>GRU + Attention</td>
                        <td>37.45%</td>
                        <td>6.89</td>
                        <td>2.14</td>
                        <td>1.10</td>
                        <td>1491</td>
                    </tr>
                    <tr>
                        <td>GRU</td>
                        <td>37.03%</td>
                        <td>5.58</td>
                        <td>1.26</td>
                        <td>1.13</td>
                        <td>1441</td>
                    </tr>
                    <tr>
                        <td>RNN + Attention</td>
                        <td>36.47%</td>
                        <td>2.69</td>
                        <td>2.12</td>
                        <td>1.10</td>
                        <td>1510</td>
                    </tr>
                    <tr>
                        <td>RNN</td>
                        <td>0.11%</td>
                        <td>1.90</td>
                        <td>1.25</td>
                        <td>4.70</td>
                        <td>59</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Visualizations -->
        <section id="visualizations">
            <h2>Performance Visualizations</h2>
            
            <div class="image-container">
                <h3>Model Comparison Overview</h3>
                <img src="/home/swapnil/Desktop/Placement26/ADAS/Transliteration/results/model_comparison.png" alt="Model Comparison">
            </div>

            <div class="image-container">
                <h3>Accuracy by Length</h3>
                <img src="/home/swapnil/Desktop/Placement26/ADAS/Transliteration/results/accuracy_by_length.png" alt="Accuracy by Length">
            </div>
            
            <div class="image-container">
                <h3>Error Distribution</h3>
                <img src="/home/swapnil/Desktop/Placement26/ADAS/Transliteration/results/error_distribution.png" alt="Error Distribution">
            </div>
            
        </section>

        <!-- Key Findings -->
        <section id="findings">
            <h2>Key Findings</h2>

            <div class="model-card">
                <h4>Transformer - Best Overall (43.07%)</h4>
                <div class="metric"><strong>Strengths:</strong> Highest accuracy, good on medium-length words (6-10 chars)</div>
                <div class="metric"><strong>Weaknesses:</strong> Catastrophic failures on 2-3 char sequences, slowest inference (15.3ms)</div>
            </div>

            <div class="model-card">
                <h4>BiLSTM + Attention - Most Reliable (42.11%)</h4>
                <div class="metric"><strong>Strengths:</strong> Lowest edit distance (0.94), fewest catastrophic errors, consistent performance</div>
                <div class="metric"><strong>Weaknesses:</strong> Largest model (37.28M params)</div>
            </div>

            <div class="model-card">
                <h4>LSTM - Best Speed/Accuracy (39.18% @ 1.29ms)</h4>
                <div class="metric"><strong>Strengths:</strong> Fast inference, decent accuracy, compact model</div>
                <div class="metric"><strong>Weaknesses:</strong> Lower accuracy than attention models</div>
            </div>
        </section>

        <!-- Worst Predictions -->
        <section id="worst-predictions">
            <h2>Worst Predictions Analysis</h2>
            
            <h3>Transformer - Catastrophic Failures on Short Sequences</h3>
            <div class="prediction-grid">
                <div class="prediction-card">
                    <strong>'pic' ‚Üí '‡§™‡§ø‡§ï'</strong><br>
                    Predicted: '‡§™‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§∏‡•Ä‡§™‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§™‡•Ä‡§Ü‡§à‡§ï‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§Ü‡§à‡§ï‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§Ü‡§à‡§∏‡•Ä‡§Ü‡§à'<br>
                    <span style="color: #dc3545;">Edit Distance: 48</span>
                </div>
                <div class="prediction-card">
                    <strong>'bk' ‚Üí '‡§¨‡•Ä‡§ï‡•á'</strong><br>
                    Predicted: '‡§¨‡§ï‡•á‡§ï‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä‡§¨‡•Ä'<br>
                    <span style="color: #dc3545;">Edit Distance: 48</span>
                </div>
                <div class="prediction-card">
                    <strong>'sk' ‚Üí '‡§è‡§∏‡§ï‡•á'</strong><br>
                    Predicted: '‡§è‡§∏‡§ï‡•á‡§ï‡•á‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä‡§è‡§∏‡•Ä'<br>
                    <span style="color: #dc3545;">Edit Distance: 46</span>
                </div>
                <div class="prediction-card">
                    <strong>'pml' ‚Üí '‡§™‡•Ä‡§è‡§Æ‡§è‡§≤'</strong><br>
                    Predicted: '‡§™‡•Ä‡§è‡§Æ‡§≤‡•Ä‡§è‡§Æ‡§è‡§™‡•Ä‡§è‡§´‡•Ä‡§è‡§´‡•Ä‡§è‡§´‡•Ä‡§è‡§´‡•Ä‡§è‡§Æ‡•Ä‡§è‡§Æ‡•Ä‡§è‡§´‡•Ä‡§è‡§´‡•Ä‡§è‡§≤‡§™‡•Ä‡§è‡§´‡•Ä‡§è‡§Æ‡•Ä‡§è‡§Æ‡•Ä‡§è‡§´'<br>
                    <span style="color: #dc3545;">Edit Distance: 44</span>
                </div>
                <div class="prediction-card">
                    <strong>'pv' ‚Üí '‡§™‡•Ä‡§µ‡•Ä'</strong><br>
                    Predicted: '‡§™‡•Ä‡§µ‡•Ä‡§µ‡•Ä‡§µ‡•Ä‡§™‡•Ä‡§µ‡•Ä‡•Ä‡•Ä‡•Ä‡•Ä‡•Ä‡•Ä‡•Ä‡§™‡•Ä‡§µ‡•Ä‡§µ‡•Ä'<br>
                    <span style="color: #dc3545;">Edit Distance: 21</span>
                </div>
            </div>

            <h3>BiLSTM + Attention - Moderate Errors</h3>
            <div class="prediction-grid">
                <div class="prediction-card">
                    <strong>'ftii' ‚Üí '‡§è‡§´‡§ü‡•Ä‡§Ü‡§à‡§Ü‡§à'</strong><br>
                    Predicted: '‡§´‡•Ä‡§ü‡•Ä'<br>
                    <span style="color: #dc3545;">Edit Distance: 6</span>
                </div>
                <div class="prediction-card">
                    <strong>'contemporary' ‚Üí '‡§ï‡§Ç‡§ü‡•á‡§Æ‡•ç‡§™‡§∞‡§∞‡•Ä'</strong><br>
                    Predicted: '‡§ï‡•â‡§®‡§ü‡•á‡§™‡•ã‡§∞‡•á'<br>
                    <span style="color: #dc3545;">Edit Distance: 6</span>
                </div>
                <div class="prediction-card">
                    <strong>'daddy' ‚Üí '‡§°‡•à‡§°‡•Ä'</strong><br>
                    Predicted: '‡§¶‡•á‡•ç‡§¶‡•ç‡§Ø'<br>
                    <span style="color: #dc3545;">Edit Distance: 6</span>
                </div>
                <div class="prediction-card">
                    <strong>'duneerti' ‚Üí '‡§¶‡•Å‡§∞‡•ç‡§®‡•Ä‡§§‡§ø'</strong><br>
                    Predicted: '‡§°‡•Å‡§®‡•Ä‡§∞‡•Ä‡§ü‡•Ä'<br>
                    <span style="color: #dc3545;">Edit Distance: 6</span>
                </div>
                <div class="prediction-card">
                    <strong>'pashtuun' ‚Üí '‡§™‡§∂‡•ç‡§§‡•Ç‡§®'</strong><br>
                    Predicted: '‡§™‡§æ‡§∑‡•ç‡§ü‡•Å‡§ì‡§Ç'<br>
                    <span style="color: #dc3545;">Edit Distance: 6</span>
                </div>
            </div>

            <h3>LSTM + Attention - Complex Word Failures</h3>
            <div class="prediction-grid">
                <div class="prediction-card">
                    <strong>'wastinghouse' ‚Üí '‡§µ‡•á‡§∏‡•ç‡§ü‡§ø‡§Ç‡§ó‡§π‡§æ‡§â‡§∏'</strong><br>
                    Predicted: '‡§µ‡§æ‡•ç‡•ç‡§ü‡§®‡§ø‡§∂‡§Ç‡§ú'<br>
                    <span style="color: #dc3545;">Edit Distance: 9</span>
                </div>
                <div class="prediction-card">
                    <strong>'tuberculosis' ‚Üí '‡§ü‡•ç‡§Ø‡•Ç‡§¨‡§∞‡§ï‡•ç‡§Ø‡•Ç‡§≤‡•ã‡§∏‡§ø‡§∏'</strong><br>
                    Predicted: '‡§ü‡§¨‡•ç‡§¨‡•Ç‡§∞‡§ï‡§≤‡§ø‡§∏'<br>
                    <span style="color: #dc3545;">Edit Distance: 8</span>
                </div>
                <div class="prediction-card">
                    <strong>'tuberkyulosis' ‚Üí '‡§ü‡•ç‡§Ø‡•Ç‡§¨‡§∞‡§ï‡•ç‡§Ø‡•Ç‡§≤‡•ã‡§∏‡§ø‡§∏'</strong><br>
                    Predicted: '‡§ü‡•ç‡§¨‡•á‡§¨‡•ç‡§∞‡•ã‡§≤‡§ø‡§ï‡§ø‡§∏'<br>
                    <span style="color: #dc3545;">Edit Distance: 8</span>
                </div>
                <div class="prediction-card">
                    <strong>'paranormal' ‚Üí '‡§™‡•à‡§∞‡§æ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤'</strong><br>
                    Predicted: '‡§™‡§∞‡§£‡•ã‡§Æ‡§æ‡§≤'<br>
                    <span style="color: #dc3545;">Edit Distance: 7</span>
                </div>
                <div class="prediction-card">
                    <strong>'westinghouse' ‚Üí '‡§µ‡•á‡§∏‡•ç‡§ü‡§ø‡§Ç‡§ó‡§π‡§æ‡§â‡§∏'</strong><br>
                    Predicted: '‡§µ‡•á‡§∏‡•ç‡§ü‡§®‡§®‡•ã‡§ú'<br>
                    <span style="color: #dc3545;">Edit Distance: 7</span>
                </div>
            </div>
        </section>

        <!-- Model Comparison Examples -->
        <section id="comparisons">
            <h2>Model Comparison: Challenging Cases</h2>
            <p>Examples where models disagreed, showing unique strengths:</p>

            <div class="comparison-table">
                <h4>'ank' ‚Üí '‡§Ö‡§Ç‡§ï'</h4>
                <span class="incorrect">‚úó rnn: '‡§™‡§æ‡§∞‡•Ä'</span><br>
                <span class="correct">‚úì lstm: '‡§Ö‡§Ç‡§ï'</span><br>
                <span class="incorrect">‚úó gru: '‡§è‡§Ç‡§ï'</span><br>
                <span class="incorrect">‚úó rnn_attention: '‡§è‡§Ç‡§ï'</span><br>
                <span class="correct">‚úì lstm_attention: '‡§Ö‡§Ç‡§ï'</span><br>
                <span class="incorrect">‚úó gru_attention: '‡§Ü‡§Ç‡§ï‡•á'</span><br>
                <span class="correct">‚úì bilstm_attention: '‡§Ö‡§Ç‡§ï'</span><br>
                <span class="correct">‚úì transformer: '‡§Ö‡§Ç‡§ï'</span>
            </div>

            <div class="comparison-table">
                <h4>'ankit' ‚Üí '‡§Ö‡§Ç‡§ï‡§ø‡§§'</h4>
                <span class="incorrect">‚úó rnn: '‡§™‡§∞‡•ç‡§∞‡•ã‡§Ç'</span><br>
                <span class="incorrect">‚úó lstm: '‡§Ö‡§Ç‡§ï‡•Ä‡§§'</span><br>
                <span class="incorrect">‚úó gru: '‡§Ö‡§Ç‡§ï‡•Ä‡§§'</span><br>
                <span class="incorrect">‚úó rnn_attention: '‡§Ö‡§Ç‡§ï‡•Ä‡§ü'</span><br>
                <span class="correct">‚úì lstm_attention: '‡§Ö‡§Ç‡§ï‡§ø‡§§'</span><br>
                <span class="incorrect">‚úó gru_attention: '‡§Ö‡§®‡§ï‡§ø‡§§'</span><br>
                <span class="correct">‚úì bilstm_attention: '‡§Ö‡§Ç‡§ï‡§ø‡§§'</span><br>
                <span class="correct">‚úì transformer: '‡§Ö‡§Ç‡§ï‡§ø‡§§'</span>
            </div>

            <div class="comparison-table">
                <h4>'angarak' ‚Üí '‡§Ö‡§Ç‡§ó‡§æ‡§∞‡§ï' (Only Transformer got it right!)</h4>
                <span class="incorrect">‚úó rnn: '‡§Ö‡§∞‡•ç‡§Æ‡§æ‡§®‡§æ'</span><br>
                <span class="incorrect">‚úó lstm: '‡§Ö‡§Ç‡§ó‡§∞‡§ï'</span><br>
                <span class="incorrect">‚úó gru: '‡§Ö‡§Ç‡§ó‡§∞‡§ï'</span><br>
                <span class="incorrect">‚úó rnn_attention: '‡§Ö‡§Ç‡§ó‡§∞‡§ï'</span><br>
                <span class="incorrect">‚úó lstm_attention: '‡§Ö‡§Ç‡§ó‡§∞‡§ï'</span><br>
                <span class="incorrect">‚úó gru_attention: '‡§Ö‡§Ç‡§ó‡§∞‡§ï'</span><br>
                <span class="incorrect">‚úó bilstm_attention: '‡§Ö‡§Ç‡§ó‡§∞‡§ï'</span><br>
                <span class="correct">‚úì transformer: '‡§Ö‡§Ç‡§ó‡§æ‡§∞‡§ï'</span>
            </div>

            <div class="comparison-table">
                <h4>'ambaani' ‚Üí '‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä'</h4>
                <span class="incorrect">‚úó rnn: '‡§Ö‡§®‡§æ‡§∞‡§æ‡§®‡•Ä'</span><br>
                <span class="incorrect">‚úó lstm: '‡§Ö‡§Æ‡§¨‡§æ‡§®‡•Ä'</span><br>
                <span class="correct">‚úì gru: '‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä'</span><br>
                <span class="incorrect">‚úó rnn_attention: '‡§Ö‡§Æ‡§¨‡§æ‡§®‡•Ä'</span><br>
                <span class="correct">‚úì lstm_attention: '‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä'</span><br>
                <span class="incorrect">‚úó gru_attention: '‡§Ö‡§Æ‡§¨‡§æ‡§®‡•Ä'</span><br>
                <span class="correct">‚úì bilstm_attention: '‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä'</span><br>
                <span class="correct">‚úì transformer: '‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä'</span>
            </div>

            <div class="comparison-table">
                <h4>'andha' ‚Üí '‡§Ö‡§Ç‡§ß‡§æ' (Easy for most models)</h4>
                <span class="incorrect">‚úó rnn: '‡§™‡§∞‡•ç‡§æ‡§æ'</span><br>
                <span class="correct">‚úì lstm: '‡§Ö‡§Ç‡§ß‡§æ'</span><br>
                <span class="correct">‚úì gru: '‡§Ö‡§Ç‡§ß‡§æ'</span><br>
                <span class="correct">‚úì rnn_attention: '‡§Ö‡§Ç‡§ß‡§æ'</span><br>
                <span class="correct">‚úì lstm_attention: '‡§Ö‡§Ç‡§ß‡§æ'</span><br>
                <span class="correct">‚úì gru_attention: '‡§Ö‡§Ç‡§ß‡§æ'</span><br>
                <span class="correct">‚úì bilstm_attention: '‡§Ö‡§Ç‡§ß‡§æ'</span><br>
                <span class="correct">‚úì transformer: '‡§Ö‡§Ç‡§ß‡§æ'</span>
            </div>
        </section>

        <!-- Common Failure Patterns -->
        <section id="failure-patterns">
            <h2>Common Failure Patterns</h2>

            <h3>1. Acronyms & Abbreviations</h3>
            <div class="error-example">
                <strong>Challenge:</strong> Models struggle with character-by-character transliteration<br>
                Examples: 'ftii', 'pic', 'bk', 'sk', 'pml'<br>
                <strong>Issue:</strong> Should transliterate as individual letters, not phonetically
            </div>

            <h3>2. English Loanwords</h3>
            <div class="error-example">
                <strong>Challenge:</strong> Complex phonetics from English origin<br>
                Examples: 'tuberculosis', 'contemporary', 'paranormal'<br>
                <strong>Issue:</strong> Requires understanding of English pronunciation rules
            </div>

            <h3>3. Consonant Clusters</h3>
            <div class="error-example">
                <strong>Challenge:</strong> Devanagari conjuncts (halant characters)<br>
                Examples: 'pashtuun' (‡§™‡§∂‡•ç‡§§‡•Ç‡§®), 'duneerti' (‡§¶‡•Å‡§∞‡•ç‡§®‡•Ä‡§§‡§ø)<br>
                <strong>Issue:</strong> Halant placement and cluster formation
            </div>

            <h3>4. Transformer Repetition Bug</h3>
            <div class="error-example">
                <strong>Challenge:</strong> Short sequences cause catastrophic repetition<br>
                Examples: 'pic', 'bk', 'sk' generate 40+ character gibberish<br>
                <strong>Root Cause:</strong> Attention mechanism feedback loop on minimal context<br>
                <strong>Solution:</strong> Use BiLSTM for sequences ‚â§ 3 characters
            </div>
        </section>

        <!-- Success Patterns -->
        <section id="success-patterns">
            <h2>Success Examples</h2>

            <div class="success-example">
                <h4>Common Hindi Words (High Success)</h4>
                'namaste' ‚Üí '‡§®‡§Æ‡§∏‡•ç‡§§‡•á' (All models except RNN)<br>
                'dhanyavaad' ‚Üí '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶' (LSTM, BiLSTM, Transformer)<br>
                'bharat' ‚Üí '‡§≠‡§æ‡§∞‡§§' (Most models)<br>
                'kripaya' ‚Üí '‡§ï‡•É‡§™‡§Ø‡§æ' (Attention models)
            </div>

            <div class="success-example">
                <h4>Medium Complexity (6-8 characters)</h4>
                'angaarak' ‚Üí '‡§Ö‡§Ç‡§ó‡§æ‡§∞‡§ï' (6/8 models correct)<br>
                'akhand' ‚Üí '‡§Ö‡§ñ‡§Ç‡§°' (5/8 models correct)<br>
                'ambaani' ‚Üí '‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä' (4/8 models correct)
            </div>
        </section>

        <!-- Dataset -->
        <section id="dataset">
            <h2>Dataset</h2>
            <p><strong>Dakshina Dataset</strong> (Google Research)</p>
            <ul>
                <li><strong>Training:</strong> 44,204 pairs</li>
                <li><strong>Validation:</strong> 4,358 pairs</li>
                <li><strong>Test:</strong> 4,502 pairs</li>
                <li><strong>Task:</strong> English (Latin script) ‚Üí Hindi (Devanagari script)</li>
            </ul>
            
            <div class="success-example">
                <strong>Example Pairs:</strong><br>
                namaste ‚Üí ‡§®‡§Æ‡§∏‡•ç‡§§‡•á<br>
                dhanyavaad ‚Üí ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶<br>
                bharat ‚Üí ‡§≠‡§æ‡§∞‡§§<br>
                sanskrit ‚Üí ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§
            </div>
        </section>

        <!-- Models -->
        <section id="models">
            <h2>Model Architectures</h2>
            
            <h3>1. Vanilla RNN/LSTM/GRU</h3>
            <p>Basic encoder-decoder architecture with different cell types. RNN failed completely (0.11%), while LSTM and GRU achieved ~37-39% accuracy.</p>

            <h3>2. RNN/LSTM/GRU + Attention</h3>
            <p>Added Bahdanau attention mechanism. Improved performance by 0-2% over vanilla versions, with better handling of longer sequences.</p>

            <h3>3. Bidirectional LSTM + Attention</h3>
            <p>BiLSTM encoder processes sequences in both directions. Achieved 42.11% accuracy with the best error profile (lowest edit distance of 0.94).</p>

            <h3>4. Transformer</h3>
            <p>Standard Transformer with positional encoding and multi-head attention (8 heads, 256 d_model). Achieved highest accuracy (43.07%) but with catastrophic failures on very short inputs (2-3 characters).</p>
        </section>

        <!-- Usage -->
        <section id="usage">
            <h2>Quick Start</h2>
            
            <h3>Installation</h3>
            <div class="code-block"><code>pip install torch pandas numpy matplotlib seaborn tqdm</code></div>

<!-- Continuation from line 723 -->

            <h3>Train All Models</h3>
            <div class="code-block"><code>python trainer_all.py</code></div>

            <h3>Generate Analysis</h3>
            <div class="code-block"><code>python analyze_results.py</code></div>

            <h3>Use Cases</h3>
            <ul>
                <li><strong>Production (Speed Critical):</strong> LSTM (39% @ 1.3ms) - Best for mobile/embedded</li>
                <li><strong>Production (Accuracy Critical):</strong> Transformer (43% @ 15ms) - Avoid sequences &lt; 4 chars</li>
                <li><strong>Research/Analysis:</strong> BiLSTM + Attention (42% @ 2.5ms, best error profile)</li>
                <li><strong>Balanced:</strong> LSTM + Attention (39% @ 2.2ms) - Good middle ground</li>
                <li><strong>Embedded/Mobile:</strong> GRU (37% @ 1.26ms) - Smallest footprint</li>
            </ul>

            <h3>Model Selection Guide</h3>
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Recommended Model</th>
                        <th>Rationale</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Real-time typing suggestions</td>
                        <td>LSTM or GRU</td>
                        <td>Fast inference required (&lt;2ms)</td>
                    </tr>
                    <tr>
                        <td>Batch processing</td>
                        <td>Transformer</td>
                        <td>Highest accuracy, speed less critical</td>
                    </tr>
                    <tr>
                        <td>Mobile keyboards</td>
                        <td>GRU</td>
                        <td>Small model size, fast inference</td>
                    </tr>
                    <tr>
                        <td>Critical applications</td>
                        <td>BiLSTM + Attention</td>
                        <td>Lowest error distance, most consistent</td>
                    </tr>
                    <tr>
                        <td>Short sequences (2-3 chars)</td>
                        <td>BiLSTM + Attention</td>
                        <td>Transformer fails catastrophically</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Implementation Details -->
        <section id="implementation">
            <h2>Implementation Details</h2>

            <h3>Training Configuration</h3>
            <ul>
                <li><strong>Embedding:</strong> 256 dimensions</li>
                <li><strong>Hidden Size:</strong> 256-512 (model dependent)</li>
                <li><strong>Layers:</strong> 2-3 layers</li>
                <li><strong>Dropout:</strong> 0.1-0.3</li>
                <li><strong>Optimizer:</strong> Adam (lr=0.001)</li>
                <li><strong>Batch Size:</strong> 64</li>
                <li><strong>Epochs:</strong> 30-50</li>
                <li><strong>Loss:</strong> Cross-Entropy with padding mask</li>
            </ul>

            <h3>Transformer Specific</h3>
            <ul>
                <li><strong>d_model:</strong> 256</li>
                <li><strong>Attention Heads:</strong> 8</li>
                <li><strong>FFN Hidden:</strong> 1024</li>
                <li><strong>Positional Encoding:</strong> Sinusoidal</li>
                <li><strong>Label Smoothing:</strong> 0.1</li>
            </ul>

            <h3>BiLSTM + Attention Specific</h3>
            <ul>
                <li><strong>Bidirectional:</strong> True (2x hidden size)</li>
                <li><strong>Attention:</strong> Bahdanau mechanism</li>
                <li><strong>Total Params:</strong> 37.28M (largest model)</li>
            </ul>
        </section>

        <!-- Performance Analysis -->
        <section id="performance">
            <h2>Performance by Sequence Length</h2>

            <h3>Short Sequences (2-4 chars)</h3>
            <div class="error-example">
                <strong>Challenge:</strong> Limited context for models<br>
                <strong>Best:</strong> BiLSTM + Attention (~37%)<br>
                <strong>Worst:</strong> Transformer (catastrophic failures, repetition bugs)<br>
                <strong>Recommendation:</strong> Avoid Transformer for very short inputs
            </div>

            <h3>Medium Sequences (5-8 chars)</h3>
            <div class="success-example">
                <strong>Sweet Spot:</strong> All models perform best here<br>
                <strong>Best:</strong> Transformer (~45%)<br>
                <strong>Average:</strong> LSTM/GRU + Attention (~40-42%)<br>
                <strong>Typical Words:</strong> namaste, bharat, hindi
            </div>

            <h3>Long Sequences (9+ chars)</h3>
            <div class="error-example">
                <strong>Challenge:</strong> Complex dependencies<br>
                <strong>Best:</strong> Transformer maintains ~45%<br>
                <strong>Declining:</strong> RNN-based models drop to ~30-35%<br>
                <strong>Reason:</strong> Vanishing gradient in RNNs, attention helps but not enough
            </div>
        </section>

        <!-- Limitations -->
        <section id="limitations">
            <h2>Known Limitations</h2>

            <h3>All Models</h3>
            <ul>
                <li>Struggle with acronyms (ftii, bk, pic)</li>
                <li>Poor on English loanwords with complex phonetics</li>
                <li>Difficulty with rare Devanagari conjuncts</li>
                <li>No explicit linguistic rules, purely data-driven</li>
                <li>Performance ceiling at ~43% suggests need for more training data or hybrid approaches</li>
            </ul>

            <h3>Transformer Specific</h3>
            <ul>
                <li><strong>Critical:</strong> Repetition bug on 2-3 character sequences</li>
                <li>Generates 40+ character gibberish from minimal input</li>
                <li>Requires input length filtering or fallback to BiLSTM</li>
                <li>Slower inference (15ms vs 1-3ms for others)</li>
            </ul>

            <h3>BiLSTM Specific</h3>
            <ul>
                <li>Large model size (37.28M params)</li>
                <li>Longer training time</li>
                <li>Higher memory requirements</li>
            </ul>

            <h3>RNN/LSTM/GRU Specific</h3>
            <ul>
                <li>Performance degrades on sequences longer than training distribution</li>
                <li>Vanilla RNN completely failed (0.11% accuracy)</li>
                <li>Limited by sequential processing (no parallelization)</li>
            </ul>
        </section>

        <!-- Future Work -->
        <section id="future-work">
            <h2>Future Work & Improvements</h2>

            <h3>Model Improvements</h3>
            <ul>
                <li><strong>Ensemble Models:</strong> Combine Transformer + BiLSTM based on input length</li>
                <li><strong>Pre-training:</strong> Use ByT5 or mT5 multilingual models</li>
                <li><strong>Data Augmentation:</strong> Synthetic data generation, back-translation</li>
                <li><strong>Hybrid Approaches:</strong> Neural + rule-based for acronyms</li>
                <li><strong>Length-aware Training:</strong> Stratified sampling by sequence length</li>
            </ul>

            <h3>Architectural Experiments</h3>
            <ul>
                <li>Transformer-XL for longer sequences</li>
                <li>Conv-Seq2Seq models</li>
                <li>Character-aware embeddings (fastText style)</li>
                <li>Multi-task learning (transliteration + phoneme prediction)</li>
            </ul>

            <h3>Evaluation Metrics</h3>
            <ul>
                <li>Phonetic similarity scores (beyond edit distance)</li>
                <li>Human evaluation on ambiguous cases</li>
                <li>Domain-specific accuracy (names vs common words)</li>
            </ul>
        </section>

        <!-- Download -->
        <section id="download" style="text-align: center;">
            <h2>Resources</h2>
            <a href="https://github.com/Swapnil-Mahajan-MS/Transliteration-models-comparative-benchmark" class="btn">üìÅ View on GitHub</a>
            <a href="/home/swapnil/Desktop/Placement26/ADAS/Transliteration/results/predictions_all.csv" class="btn">üìä Download Predictions</a>
            <a href="/home/swapnil/Desktop/Placement26/ADAS/Transliteration/results/detailed_statistics.csv" class="btn">üìà Download Statistics</a>
            <a href="/home/swapnil/Desktop/Placement26/ADAS/Transliteration/results/model_comparison.png" class="btn">üñºÔ∏è Download Charts</a>
        </section>

        <!-- Citation -->
        <section id="citation">
            <h2>Citation</h2>
            <div class="code-block"><code>@misc{transliteration-benchmark-2025,
  title={Sequence-to-Sequence Models for English-Hindi Transliteration: 
         A Comprehensive Benchmark},
  author={Your Name},
  year={2025},
  note={Benchmark of 8 neural architectures on Dakshina dataset},
  howpublished={\url{https://github.com/yourusername/transliteration-benchmark}}
}</code></div>

            <h3>References</h3>
            <ul style="line-height: 2;">
                <li>Dakshina Dataset: <a href="https://github.com/google-research-datasets/dakshina">Google Research</a></li>
                <li>Bahdanau Attention: <a href="https://arxiv.org/abs/1409.0473">Bahdanau et al., 2014</a></li>
                <li>Transformer: <a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a></li>
                <li>Sequence to Sequence Learning: <a href="https://arxiv.org/abs/1409.3215">Sutskever et al., 2014</a></li>
            </ul>
        </section>

        <!-- Contact -->
        <section id="contact" style="text-align: center;">
            <h2>Contact & Contributions</h2>
            <p>Found a bug? Have suggestions? Want to contribute?</p>
            <a href="https://github.com/yourusername/transliteration-benchmark/issues" class="btn">Report Issue</a>
            <a href="https://github.com/yourusername/transliteration-benchmark/pulls" class="btn">Submit PR</a>
        </section>
    </div>

    <footer>
        <p><strong>English-Hindi Transliteration Benchmark</strong> | 2025</p>
        <p>Built with PyTorch | Dakshina Dataset by Google Research</p>
        <p style="margin-top: 20px; font-size: 0.9em;">
            8 Models | 4,502 Test Samples | 36,016 Predictions Analyzed
        </p>
    </footer>
</body>
</html>
